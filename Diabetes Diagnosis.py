# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FSb7JHtQtY-rlquDqgwrZkpDzYPHJqdE

**Connection with colab**
"""

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

downloaded = drive.CreateFile({'id':'1w7DTbCQ7vndxMJpB906L634uZuWR1XQe'}) # replace the id with id of file you want to access
downloaded.GetContentFile('diabetes.csv')

"""**Import all the python library**"""

import pandas as pd
import numpy as np   
import seaborn as sns
import matplotlib.pyplot as plt

"""**Data processing**"""

data_frame = pd.read_csv('diabetes.csv')

data_frame.shape

data_frame.head(3)

data_frame.tail(3)

data_frame.dtypes

"""**Missing data chicking**"""

print(data_frame.isnull().values.any())                  #dataset e kono faka set ache ki na ta dekhar jonne

data_frame.corr() # It will show correlation matrix/confusion matrix

data_frame.describe()

data_frame['Outcome'].value_counts()

"""**Data visualization**"""

sns.countplot(data_frame['Outcome'])

# create four distplots
plt.figure(figsize=(12,10))
plt.subplot(221)
sns.distplot(data_frame[data_frame['Outcome']==0].Age)
plt.title('Age of patients without diabetes disease')
plt.subplot(222)
sns.distplot(data_frame[data_frame['Outcome']==1].Age)
plt.title('Age of patients with diabetes disease')

plt.figure(figsize=(12,10))
plt.subplot(223)
sns.distplot(data_frame[data_frame['Outcome']==0].BMI)
plt.title('Max over weight of patients without diabetes disease')
plt.subplot(224)
sns.distplot(data_frame[data_frame['Outcome']==1].BMI)
plt.title('Max over weight of patients with diabetes disease')
plt.show()

data_frame.hist(figsize=(14,14))
plt.show()

# Pairplot 
sns.pairplot(data = data_frame, hue = 'Outcome')
plt.show()

# Here size means plot-size
def corr_heatmap(data_frame, size=11):
  # Getting correlation using Pandas
  correlation = data_frame.corr()

  # Dividing the plot into subplots for increasing size of plots
  fig, heatmap = plt.subplots(figsize=(size, size))

  # Plotting the correlation heatmap
  heatmap.matshow(correlation)

  # Adding xticks and yticks
  plt.xticks(range(len(correlation.columns)), correlation.columns)
  plt.yticks(range(len(correlation.columns)), correlation.columns)

  # Displaying the graph
  plt.show()

corr_heatmap(data_frame,12)

plt.figure(figsize=(12,6))
corr = data_frame.corr()
sns.heatmap(corr,annot=True, fmt ='.3f',linewidths=.5, cmap='summer')
plt.show()

"""**Split the dataset**"""

#from sklearn.cross_validation import train_test_split
from sklearn.model_selection import train_test_split

feature_column_names = ['Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age']

predicted_class_name = ['Outcome']


# Getting feature variable values

X = data_frame[feature_column_names].values
y = data_frame[predicted_class_name].values

# Saving 30% for testing
split_test_size = 0.10

# Splitting using scikit-learn train_test_split function

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = split_test_size, random_state = 60)

print("{:.2f}% in training set".format((len(X_train)/len(data_frame.index)) * 100))
print("{:.2f}% in test set".format((len(X_test)/len(data_frame.index)) * 100))

"""**Missing data chicking**"""

print("# rows in dataframe {0}".format(len(data_frame)))
print("# rows missing Glucose: {0}".format(len(data_frame.loc[data_frame['Glucose'] == 0])))
print("# rows missing BloodPressure: {0}".format(len(data_frame.loc[data_frame['BloodPressure'] == 0])))
print("# rows missing Thickness: {0}".format(len(data_frame.loc[data_frame['SkinThickness'] == 0])))
print("# rows missing Insulin: {0}".format(len(data_frame.loc[data_frame['Insulin'] == 0])))
print("# rows missing BMI: {0}".format(len(data_frame.loc[data_frame['BMI'] == 0])))
print("# rows missing DiabetesPedigreeFunction: {0}".format(len(data_frame.loc[data_frame['DiabetesPedigreeFunction'] == 0])))
print("# rows missing Age: {0}".format(len(data_frame.loc[data_frame['Age'] == 0])))

"""**Missing data filled up**"""

#from sklearn.preprocessing import Imputer
#from sklearn.impute import SimpleImputer
#Impute with mean all 0 readings
from sklearn.impute import SimpleImputer
fill_0 = SimpleImputer(missing_values=0, strategy="mean")
X_train = fill_0.fit_transform(X_train)
X_test = fill_0.fit_transform(X_test)

X_train[:10]

"""**Select the right machine learning algorithm**

**Decision tree Algorithm**
"""

# Decision tree Algorithm
from sklearn.tree import DecisionTreeClassifier
decisionTree_model = DecisionTreeClassifier(random_state =42)
decisionTree_model.fit(X_train, y_train.ravel())


#decisionTree= DecisionTreeClassifier(criterion = 'gini', random_state = 80)

from sklearn import metrics
decisionTree_predict_train = decisionTree_model.predict(X_train)

#get accuracy
decisionTree_accuracy = metrics.accuracy_score(y_train, decisionTree_predict_train)

#print accuracy
print ("Accuracy: {0:.4f}".format(decisionTree_accuracy))

decisionTree_predict_test = decisionTree_model.predict(X_test)

#get accuracy
decisionTree_accuracy_testdata = metrics.accuracy_score(y_test, decisionTree_predict_test)

#print accuracy
print ("Accuracy: {0:.4f}".format(decisionTree_accuracy_testdata))

print("Confusion Matrix")

print("{0}".format(metrics.confusion_matrix(y_test,decisionTree_predict_test, labels = [1,0])))
print(" ")

print(" Classification Report ")
print(metrics.classification_report(y_test,decisionTree_predict_test, labels = [1,0]))

"""**Random forest**

"""

from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout

from sklearn.ensemble import RandomForestClassifier

# Create a RandomForestClassifier object
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train.ravel())

rf_predict_train = rf_model.predict(X_train)

#get accuracy
rf_accuracy = metrics.accuracy_score(y_train, rf_predict_train)

#print accuracy
print ("Accuracy: {0:.4f}".format(rf_accuracy))

rf_predict_test = rf_model.predict(X_test)

#get accuracy
rf_accuracy_testdata = metrics.accuracy_score(y_test, rf_predict_test)

#print accuracy
print ("Accuracy: {0:.4f}".format(rf_accuracy_testdata))

print("Confusion Matrix")

print("{0}".format(metrics.confusion_matrix(y_test, rf_predict_test, labels = [1,0])))
print(" ")

print(" Classification Report ")
print(metrics.classification_report(y_test, rf_predict_test, labels = [1,0]))

"""**Logistic Regrassion**"""

from sklearn.linear_model import LogisticRegression

# Create a RandomForestClassifier object
lr_model = LogisticRegression(random_state=42)

lr_model.fit(X_train, y_train.ravel())

lr_predict_train = lr_model.predict(X_train)

#get accuracy
lr_accuracy = metrics.accuracy_score(y_train, lr_predict_train)

#print accuracy
print ("Accuracy: {0:.4f}".format(lr_accuracy))

lr_predict_test = lr_model.predict(X_test)

#get accuracy
lr_accuracy_testdata = metrics.accuracy_score(y_test, lr_predict_test)

#print accuracy
print ("Accuracy: {0:.4f}".format(lr_accuracy_testdata))

print("Confusion Matrix")

print("{0}".format(metrics.confusion_matrix(y_test, rf_predict_test, labels = [1,0])))
print(" ")

print(" Classification Report ")
print(metrics.classification_report(y_test, rf_predict_test, labels = [1,0]))

"""** K nearest neighbors Algorithm**"""

# K nearest neighbors Algorithm
from sklearn.neighbors import KNeighborsClassifier
knn_model= KNeighborsClassifier(n_neighbors = 24, metric = 'minkowski', p = 2)
knn_model.fit(X_train, y_train.ravel())

knn_predict_train = knn_model.predict(X_train)

#get accuracy
knn_accuracy = metrics.accuracy_score(y_train, knn_predict_train)

#print accuracy
print ("Accuracy: {0:.4f}".format(knn_accuracy))

knn_predict_test = knn_model.predict(X_test)

#get accuracy
knn_accuracy_testdata = metrics.accuracy_score(y_test, knn_predict_test)

#print accuracy
print ("Accuracy: {0:.4f}".format(knn_accuracy_testdata))

print("Confusion Matrix")

print("{0}".format(metrics.confusion_matrix(y_test, rf_predict_test, labels = [1,0])))
print(" ")

print(" Classification Report ")
print(metrics.classification_report(y_test, rf_predict_test, labels = [1,0]))

"""**Evaluating for multiple models**"""

from sklearn.calibration import calibration_curve

def model_comparison(X_train,X_test, y_train,y_test):
 
    lr = LogisticRegression()
  
    rf = RandomForestClassifier(n_estimators=100)
    
    plt.figure(figsize=(10, 10))
    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)
    ax2 = plt.subplot2grid((3, 1), (2, 0))
    
    ax1.plot([0, 1], [0, 1], "k:", label="Perfectly calibrated")
    for clf, name in [(lr, 'Logistic'),\
                      
                      (rf, 'Random Forest')]:
        clf.fit(X_train, y_train)
        if hasattr(clf, "predict_proba"):
            prob_pos = clf.predict_proba(X_test)[:, 1]
        else:  # use decision function
            prob_pos = clf.decision_function(X_test)
            prob_pos = \
                (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())
        fraction_of_positives, mean_predicted_value = \
            calibration_curve(y_test, prob_pos, n_bins=10)
    
        ax1.plot(mean_predicted_value, fraction_of_positives, "s-",
                 label="%s" % (name, ))
    
        ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,
                 histtype="step", lw=2)
    
    ax1.set_ylabel("Fraction of positives")
    ax1.set_ylim([-0.05, 1.05])
    ax1.legend(loc="lower right")
    ax1.set_title('Calibration plots  (reliability curve)')
    
    ax2.set_xlabel("Mean predicted value")
    ax2.set_ylabel("Count")
    ax2.legend(loc="upper center", ncol=2)
    
    plt.tight_layout()
    plt.show()

model_comparison(X_train, X_test, y_train, y_test)

